Y:\nfs\ubuntu_test\linux\kernel\cgroup\cgroup.c

```c
#define SUBSYS(_x) [_x ## _cgrp_id] = &_x ## _cgrp_subsys,
struct cgroup_subsys *cgroup_subsys[] = {
#include <linux/cgroup_subsys.h>    ->    SUBSYS(memory)
};
```

Y:\nfs\ubuntu_test\linux\include\linux\cgroup-defs.h

```c
#define SUBSYS(_x) _x ## _cgrp_id,
enum cgroup_subsys_id {
#include <linux/cgroup_subsys.h>
    CGROUP_SUBSYS_COUNT,
};
#undef SUBSYS
```

展开后

```c
struct cgroup_subsys *cgroup_subsys[] = {
[memory_cgrp_id] = &memory_cgrp_subsys,
}；
```

**cgroup_init_early**

1，完成cgrp_dfl_root的初始化，该cgroup_root为默认的hierarchy，只包含一个cgroup，所有未attached子系统与其相关，所有进程初始均包含在该cgroup中

2，初始化init_task的cgroups成员，其控制信息初始化为init_css_set，在其他hierarchy挂载之前默认使用

3，初始化subsys，设置其id和name

**cgroup_init**

完成其他初始化工作

Y:\nfs\ubuntu_test\linux\mm\memcontrol.c mem_cgroup_init 初始化每个内存节点的soft_limit_tree红黑树

```c
// cftype: 用于定义和描述控制组的控制文件
// cftype->private：描述资源类型和资源属性
// dfl_cftypes和legacy_cftypes都是cftype的成员
struct cgroup_subsys memory_cgrp_subsys = {    //memcg子系统控制描述符
    .css_alloc = mem_cgroup_css_alloc,   //初始化
    .css_online = mem_cgroup_css_online,
    .css_offline = mem_cgroup_css_offline,
    .css_released = mem_cgroup_css_released,
    .css_free = mem_cgroup_css_free,
    .css_reset = mem_cgroup_css_reset,
    .css_rstat_flush = mem_cgroup_css_rstat_flush,
    .can_attach = mem_cgroup_can_attach,
    .attach = mem_cgroup_attach,
    .cancel_attach = mem_cgroup_cancel_attach,
    .post_attach = mem_cgroup_move_task,
    .dfl_cftypes = memory_files,    //默认层级
    .legacy_cftypes = mem_cgroup_legacy_files,//子层级 在/sys/fs/cgroup/memory目录下看到子层级的限制
    .early_init = 0,
};

static struct mem_cgroup *mem_cgroup_alloc(void)
{
    struct mem_cgroup *memcg;
    int node;
    int __maybe_unused i;
    long error = -ENOMEM;

    memcg = kzalloc(struct_size(memcg, nodeinfo, nr_node_ids), GFP_KERNEL);
    if (!memcg)
        return ERR_PTR(error);

    memcg->id.id = idr_alloc(&mem_cgroup_idr, NULL,
                 1, MEM_CGROUP_ID_MAX + 1, GFP_KERNEL);
    if (memcg->id.id < 0) {
        error = memcg->id.id;
        goto fail;
    }

    memcg->vmstats = kzalloc(sizeof(struct memcg_vmstats), GFP_KERNEL);
    if (!memcg->vmstats)
        goto fail;

    memcg->vmstats_percpu = alloc_percpu_gfp(struct memcg_vmstats_percpu,
                         GFP_KERNEL_ACCOUNT);
    if (!memcg->vmstats_percpu)
        goto fail;

    for_each_node(node)
        if (alloc_mem_cgroup_per_node_info(memcg, node))
            goto fail;

    if (memcg_wb_domain_init(memcg, GFP_KERNEL))
        goto fail;

    INIT_WORK(&memcg->high_work, high_work_func);
    INIT_LIST_HEAD(&memcg->oom_notify);
    mutex_init(&memcg->thresholds_lock);
    spin_lock_init(&memcg->move_lock);
    vmpressure_init(&memcg->vmpressure);
    INIT_LIST_HEAD(&memcg->event_list);
    spin_lock_init(&memcg->event_list_lock);
    memcg->socket_pressure = jiffies;
#ifdef CONFIG_MEMCG_KMEM
    memcg->kmemcg_id = -1;
    INIT_LIST_HEAD(&memcg->objcg_list);
#endif
#ifdef CONFIG_CGROUP_WRITEBACK
    INIT_LIST_HEAD(&memcg->cgwb_list);
    for (i = 0; i < MEMCG_CGWB_FRN_CNT; i++)
        memcg->cgwb_frn[i].done =
            __WB_COMPLETION_INIT(&memcg_cgwb_frn_waitq);
#endif
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
    spin_lock_init(&memcg->deferred_split_queue.split_queue_lock);
    INIT_LIST_HEAD(&memcg->deferred_split_queue.split_queue);
    memcg->deferred_split_queue.split_queue_len = 0;
#endif
    idr_replace(&mem_cgroup_idr, memcg, memcg->id.id);
    lru_gen_init_memcg(memcg);
    return memcg;
fail:
    mem_cgroup_id_remove(memcg);
    __mem_cgroup_free(memcg);
    return ERR_PTR(error);
}
```

mem_cgroup_css_alloc
