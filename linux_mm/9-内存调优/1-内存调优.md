**1，针对NUMA系统**
/*

* Both the MPOL_* mempolicy mode and the MPOL_F_* optional mode flags are
* passed by the user to either set_mempolicy() or mbind() in an 'int' actual.
* The MPOL_MODE_FLAGS macro determines the legal set of optional mode flags.
  */

/* Policies */
enum {
    MPOL_DEFAULT,
    MPOL_PREFERRED,
    MPOL_BIND,
    MPOL_INTERLEAVE,
    MPOL_LOCAL,
    MPOL_PREFERRED_MANY,
    MPOL_MAX,    /* always last member of enum */
};

**2，madvise**

madvise 调整那些区域使用THP

madvise 调整那些区域使用KSM

**3， 调整lru cache的大小**

**4，/proc/sys/vm/page-cluster 基于磁盘的局部性原理**

page-cluster的作用就是每次从交换设备读取数据时多读 2^n 页，一些块设备或文件系统有簇的概念，读取数据也是按簇读取，假设内存页大小为 4KiB，page-cluster为3的情况每次读取的一簇数据为 32KiB，这是swap时利用局部性原理，把附近的几个页“顺便”也读出来，减少频繁读取磁盘的次数。而其实使用zRAM时，并不会有磁盘相关操作，这里多读一些甚至是浪费内存的。而且压缩后在zram中的数据也并不一定具有局部性原理。这里是可以通过实际测试来判断的。

**5，/sys/kernel/mm/swap/vma_ra_enabled** 

基于VMA的局部性原理，比较老的内核不支持

**6，/proc/sys/vm/swappiness**
swappiness参数用于表征更倾向于回收匿名页还是文件页。Linux5.8以下版本swapiness默认值是60，最大值是100，如果swappiness设置为100表示匿名页和文件将用同样的优先级进行回收。由于zram设备非外部存储设备，其本质还是对RAM的访问，可以相对更激进地使用zram，即把swappiness设置得比100还大，也就是回收匿名页比文件页激进。

**7，/sys/block/zram0/ dedup_enable**
重复页去重

**8，利用内核fragmentation index优化内存碎片**

cat /proc/sys/vm/extfrag_threshold
500
```sh
# cat /sys/kernel/debug/extfrag/extfrag_index
Node 0, zone   Normal -1.000 -1.000 -1.000 -1.000 0.792 0.893 0.944 0.969 0.982 0.988 0.991
```

fragmentation_index计算公式如下

```textile
frag_index=1000 -((1000+(free_pages*1000)/request)/free_block_total)
free_pages  剩余空闲页数量
request     要申请的页数
free_block_total   空闲的连续页块
在free_page不变的条件下，free_block_total越多，证明每个free_block包含的连续页越小
证明内存随便越严重，那么frag_index也就越大
因此，frag_index约接近0，表明申请不到内存是因为缺少内存，frag_index越接近1000，表明
申请不到内存是因为内存碎片化
```
默认碎片指数大于500时才能进行compact（内存规整）

**9利用ZONE_MOVABLE优化内存碎片**

它存在的意义实际上是为了减少内存的碎片化，想象一下这个场景，当我们需要一块大的连续内存时向伙伴系统申请，虽然对应的内存区剩余内存还很多，但是却发现对应的内存区并无法满足连续的内存申请需求，这就是由于内存碎片化导致的问题。那么此时是可以通过内存迁移来完成连续内存的申请，但是这个过程并不一定能够成功，因为中间有一些页面可能是不允许迁移的。

引入ZONE_MOVABLE就是为了优化内存迁移场景的，主要目的是想要把Non-Movable和Movable的内存区分管理，当我们划分出该区域后，那么只有可迁移的页才能够从该区域申请，这样当我们后面回收内存时，针对该区域就都可以执行迁移，从而保证能够获取到足够大的连续内存。

除了这个用途之外，该区域还有一种使用场景，那就是memory hotplug场景，内存热插拔场景，当我们对内存区执行remove时，必须保证其中的内容都是可以被迁移走的，因此热插拔的内存区必须位于ZONE_MOVABLE区域
**10 /proc 下内存监控信息文件**

```textile
cat /proc/buddyinfo
cat /proc/meminfo
ls /proc/iomem
ls /proc/pagetypeinfo
ls /proc/slabinfo
ls /proc/vmallocinfo
ls /proc/vmstat
ls /proc/zoneinfo
```

**11 设置一个进程的VMA数量**

内核规定一个进程虚拟内存空间内所能映射的虚拟内存区域 vma 是有数量限制的，sysctl_max_map_count 规定了进程虚拟内存空间所能包含 VMA 的最大个数，我们可以通过 `/proc/sys/vm/max_map_count` 内核参数来调整 sysctl_max_map_count。

**12 设置系统被mlock主的内存限额**
我们可以通过修改 `/etc/security/limits.conf` 文件中的 memlock 相关配置项来调整能够被锁定的内存资源配额，设置为 unlimited 表示不对锁定内存进行限制

**13 内核对mmap虚拟内存申请的限制**
内核对虚拟内存使用的审计策略定义在 sysctl_overcommit_memory 中，我们可以通过内核参数 `/proc/sys/vm/overcommit_memory` 来调整
内核定义了如下三个 overcommit 策略，这里的 commit 意思是需要申请的虚拟内存，overcommit 的意思是向内核申请过量的（远远超过物理内存容量）虚拟内存：
```c
#define OVERCOMMIT_GUESS        0
#define OVERCOMMIT_ALWAYS        1
#define OVERCOMMIT_NEVER        2
```
OVERCOMMIT_GUESS 是内核的默认 overcommit 策略。在这种模式下，特别激进的，过量的虚拟内存申请将会被拒绝，内核会对虚拟内存能够过量申请多少做出一定的限制，这种策略既不激进也不保守，比较中庸。
OVERCOMMIT_ALWAYS 是最为激进的 overcommit 策略，无论进程申请多大的虚拟内存，只要不超过整个进程虚拟内存空间的大小，内核总会痛快的答应。但是这种策略下，虚拟内存的申请虽然容易了，但是当进程遇到缺页，内核为其分配物理内存的时候，会非常容易造成 OOM 。
OVERCOMMIT_NEVER 是最为严格的一种控制虚拟内存 overcommit 的策略，在这种模式下，内核会严格的规定虚拟内存的申请用量。

但是当我们在 mmap 系统调用参数 flags 中设置了 MAP_NORESERVE，则内核在分配虚拟内存的时候将不会考虑物理内存的总体容量以及 swap space 的限制因素，无论申请多大的虚拟内存，内核都会满足。但缺页的时候会容易导致 oom。

MAP_NORESERVE 只会在 OVERCOMMIT_GUESS 和 OVERCOMMIT_ALWAYS 模式下才有意义，因为如果内核本身是禁止 overcommit 的话，设置 MAP_NORESERVE 是无意义的。